{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### US Demographic and Immigration Data\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "The goal of this project is to construct a pipeline for building a data lake in S3 with Spark for immigration data in the US. Three data sources are used:\n",
    "\n",
    "* I94 Immigration Data\n",
    "* Airport Codes Data\n",
    "* Country Code Data\n",
    "* USA City Demographic Data\n",
    "\n",
    "What is the use of this pipeline and who can use it? This pipeline was actually conceptualised in built from a perspective that it can house immigration data across different countries and not just the US. For the sake of this project, we are confining our dataset to just the US but other supporting data sources (airport data, country code data, city demographic data) either include data points outside the US or can be easily extended to do so. Ths pipeline mainly aims to house immigration records, supported by data that describe more about the location i.e the airport, the country, the city. We can perform analytics to find immigration records by airport type, immigration records by immigrant country, airport types by city, immigrant type by population and so on. This data can hopefully aim to aid in policy or pro-immigrant measures based on various factors that we consider. As such, data analysts, statisticians and public officials can use it to gain more insight into immigration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "import pandas\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import udf, monotonically_increasing_id, col\n",
    "from pyspark.sql import types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://udacity-data/capstone/immigration/sas_data/, s3a://udacity-data/capstone/immigration/I94_SAS_LABELS_Descriptions.SAS\n",
      "s3a://udacity-data/capstone/airport/airport-codes_csv.csv\n",
      "s3a://udacity-data/capstone/demographic/us-cities-demographics.csv\n",
      "s3a://udacity-data/capstone/output/\n",
      "s3a://udacity-data/capstone/country_codes/country_codes_i94_clean.csv, s3a://udacity-data/capstone/country_codes/country_codes.csv\n"
     ]
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "config.read_file(open('config.cfg'))\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "bucket = config['S3']['BUCKET']\n",
    "immigration_path = bucket + config['S3']['IMMIGRATION_DATA']\n",
    "immigration_labels = bucket + config['S3']['IMMIGRATION_LABELS']\n",
    "airport_path = bucket + config['S3']['AIRPORT_CODES_DATA']\n",
    "city_path = bucket + config['S3']['CITY_DATA']\n",
    "output_path = bucket + config['S3']['OUTPUT']\n",
    "country_i94_path = bucket + config['S3']['COUNTRY_I94_DATA']\n",
    "country_path = bucket + config['S3']['COUNTRY_DATA']\n",
    "\n",
    "print(f'{immigration_path}, {immigration_labels}')\n",
    "print(f'{airport_path}')\n",
    "print(f'{city_path}')\n",
    "print(f'{output_path}')\n",
    "print(f'{country_i94_path}, {country_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON']='/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']='/usr/bin/python3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.74:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.6</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f6ba4ce0ef0>"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.config('spark.jars.packages', \n",
    "                                    'org.apache.hadoop:hadoop-aws:2.7.0').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\",\n",
    "                                     os.environ['AWS_ACCESS_KEY_ID'])\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \n",
    "                                     os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time():\n",
    "    n = datetime.now()\n",
    "\n",
    "    timestamp = f\"\"\"\n",
    "    {n.year}-{n.month}-{n.day}_{n.hour}-{n.minute}-{n.second}-{n.microsecond}\n",
    "    \"\"\".strip()\n",
    "    \n",
    "    return timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_data = spark.read.parquet(immigration_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " cicid    | 5748517.0      \n",
      " i94yr    | 2016.0         \n",
      " i94mon   | 4.0            \n",
      " i94cit   | 245.0          \n",
      " i94res   | 438.0          \n",
      " i94port  | LOS            \n",
      " arrdate  | 20574.0        \n",
      " i94mode  | 1.0            \n",
      " i94addr  | CA             \n",
      " depdate  | 20582.0        \n",
      " i94bir   | 40.0           \n",
      " i94visa  | 1.0            \n",
      " count    | 1.0            \n",
      " dtadfile | 20160430       \n",
      " visapost | SYD            \n",
      " occup    | null           \n",
      " entdepa  | G              \n",
      " entdepd  | O              \n",
      " entdepu  | null           \n",
      " matflag  | M              \n",
      " biryear  | 1976.0         \n",
      " dtaddto  | 10292016       \n",
      " gender   | F              \n",
      " insnum   | null           \n",
      " airline  | QF             \n",
      " admnum   | 9.495387003E10 \n",
      " fltno    | 00011          \n",
      " visatype | B1             \n",
      "-RECORD 1------------------\n",
      " cicid    | 5748518.0      \n",
      " i94yr    | 2016.0         \n",
      " i94mon   | 4.0            \n",
      " i94cit   | 245.0          \n",
      " i94res   | 438.0          \n",
      " i94port  | LOS            \n",
      " arrdate  | 20574.0        \n",
      " i94mode  | 1.0            \n",
      " i94addr  | NV             \n",
      " depdate  | 20591.0        \n",
      " i94bir   | 32.0           \n",
      " i94visa  | 1.0            \n",
      " count    | 1.0            \n",
      " dtadfile | 20160430       \n",
      " visapost | SYD            \n",
      " occup    | null           \n",
      " entdepa  | G              \n",
      " entdepd  | O              \n",
      " entdepu  | null           \n",
      " matflag  | M              \n",
      " biryear  | 1984.0         \n",
      " dtaddto  | 10292016       \n",
      " gender   | F              \n",
      " insnum   | null           \n",
      " airline  | VA             \n",
      " admnum   | 9.495562283E10 \n",
      " fltno    | 00007          \n",
      " visatype | B1             \n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_data.limit(2).show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_data = spark.read.csv(airport_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------\n",
      " ident        | 00A                                \n",
      " type         | heliport                           \n",
      " name         | Total Rf Heliport                  \n",
      " elevation_ft | 11                                 \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-PA                              \n",
      " municipality | Bensalem                           \n",
      " gps_code     | 00A                                \n",
      " iata_code    | null                               \n",
      " local_code   | 00A                                \n",
      " coordinates  | -74.93360137939453, 40.07080078125 \n",
      "-RECORD 1------------------------------------------\n",
      " ident        | 00AA                               \n",
      " type         | small_airport                      \n",
      " name         | Aero B Ranch Airport               \n",
      " elevation_ft | 3435                               \n",
      " continent    | NA                                 \n",
      " iso_country  | US                                 \n",
      " iso_region   | US-KS                              \n",
      " municipality | Leoti                              \n",
      " gps_code     | 00AA                               \n",
      " iata_code    | null                               \n",
      " local_code   | 00AA                               \n",
      " coordinates  | -101.473911, 38.704022             \n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_data.limit(2).show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_i94_data = spark.read.csv(country_i94_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------\n",
      " _c0                    | 0           \n",
      " i94cit_clean           | 582         \n",
      " i94_country_name_clean | MEXICO      \n",
      " iso_country_code_clean | 484         \n",
      "-RECORD 1-----------------------------\n",
      " _c0                    | 1           \n",
      " i94cit_clean           | 236         \n",
      " i94_country_name_clean | AFGHANISTAN \n",
      " iso_country_code_clean | 4           \n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_i94_data.limit(2).show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_data = spark.read.csv(country_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------------------------\n",
      " name                     | Afghanistan     \n",
      " alpha-2                  | AF              \n",
      " alpha-3                  | AFG             \n",
      " country-code             | 004             \n",
      " iso_3166-2               | ISO 3166-2:AF   \n",
      " region                   | Asia            \n",
      " sub-region               | Southern Asia   \n",
      " intermediate-region      | null            \n",
      " region-code              | 142             \n",
      " sub-region-code          | 034             \n",
      " intermediate-region-code | null            \n",
      "-RECORD 1-----------------------------------\n",
      " name                     | Åland Islands   \n",
      " alpha-2                  | AX              \n",
      " alpha-3                  | ALA             \n",
      " country-code             | 248             \n",
      " iso_3166-2               | ISO 3166-2:AX   \n",
      " region                   | Europe          \n",
      " sub-region               | Northern Europe \n",
      " intermediate-region      | null            \n",
      " region-code              | 150             \n",
      " sub-region-code          | 154             \n",
      " intermediate-region-code | null            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_data.limit(2).show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_data = spark.read.option('delimiter', ';').csv(city_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " City                   | Silver Spring      \n",
      " State                  | Maryland           \n",
      " Median Age             | 33.8               \n",
      " Male Population        | 40601              \n",
      " Female Population      | 41862              \n",
      " Total Population       | 82463              \n",
      " Number of Veterans     | 1562               \n",
      " Foreign-born           | 30908              \n",
      " Average Household Size | 2.6                \n",
      " State Code             | MD                 \n",
      " Race                   | Hispanic or Latino \n",
      " Count                  | 25924              \n",
      "-RECORD 1------------------------------------\n",
      " City                   | Quincy             \n",
      " State                  | Massachusetts      \n",
      " Median Age             | 41.0               \n",
      " Male Population        | 44129              \n",
      " Female Population      | 49500              \n",
      " Total Population       | 93629              \n",
      " Number of Veterans     | 4147               \n",
      " Foreign-born           | 32935              \n",
      " Average Household Size | 2.39               \n",
      " State Code             | MA                 \n",
      " Race                   | White              \n",
      " Count                  | 58723              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "city_data.limit(2).show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Define the Data Model\n",
    "### Conceptual Data Model\n",
    "##### Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "The idea behind this pipeline is to combine different sources of data for the following purposes:\n",
    "\n",
    "* Track immigration by airport in the USA\n",
    "* Store details about each of those airports\n",
    "* Store details about the countries that USA gets immigrants from\n",
    "* Parse cities/states from airport data and store demographic data for those cities\n",
    "\n",
    "As such, the model would have a fact table and multiple dimension tables, with the fact table being the immigration data itself. This data would hold records to:\n",
    "\n",
    "* Uniquely identify each arrival\n",
    "* Hold mappings to dimensions for airport and country\n",
    "* Hold additional data about the entry like visa type\n",
    "\n",
    "The airpot, city and country dimension tables would contain additional information, not limited to the state/city the airports are in.\n",
    "\n",
    "For further reference, a schema diagram is included in the documentation.\n",
    "\n",
    "\n",
    "##### Mapping Out Data Pipelines\n",
    "\n",
    "\n",
    "1. Save initial datasets/sources to S3 buckets\n",
    "2. Load datasets into memory with Spark\n",
    "3. Create dimension tables for airports, countries, states\n",
    "4. Create the fact table for immigration, hence creating a mapping between each immigration record and its airport and country\n",
    "5. Perform data quality checks\n",
    "6. Save data as .parquet to S3 buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building out the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `countries` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- i94cit_clean: string (nullable = true)\n",
      " |-- i94_country_name_clean: string (nullable = true)\n",
      " |-- iso_country_code_clean: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_i94_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- alpha-2: string (nullable = true)\n",
      " |-- alpha-3: string (nullable = true)\n",
      " |-- country-code: string (nullable = true)\n",
      " |-- iso_3166-2: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- sub-region: string (nullable = true)\n",
      " |-- intermediate-region: string (nullable = true)\n",
      " |-- region-code: string (nullable = true)\n",
      " |-- sub-region-code: string (nullable = true)\n",
      " |-- intermediate-region-code: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_i94_data.createOrReplaceTempView('country_i94_data')\n",
    "country_data.createOrReplaceTempView('country_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_table = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "    i.i94cit_clean AS i94_country_id,\n",
    "    c.`country-code` AS iso_country_id,\n",
    "    c.name AS country_name,\n",
    "    c.region AS region,\n",
    "    c.`sub-region` AS sub_region,\n",
    "    c.`alpha-2` AS iso_country_alpha\n",
    "    FROM country_data AS c\n",
    "    LEFT OUTER JOIN country_i94_data AS i ON\n",
    "        i.iso_country_code_clean = c.`country-code`\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+-------------+------+---------------+-----------------+\n",
      "|i94_country_id|iso_country_id| country_name|region|     sub_region|iso_country_alpha|\n",
      "+--------------+--------------+-------------+------+---------------+-----------------+\n",
      "|          null|           004|  Afghanistan|  Asia|  Southern Asia|               AF|\n",
      "|          null|           248|Åland Islands|Europe|Northern Europe|               AX|\n",
      "+--------------+--------------+-------------+------+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries_table.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_table.createOrReplaceTempView('countries_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing airports_table to s3a://udacity-data/capstone/output/countries_table_2020-6-21_3-38-47-624686..\n"
     ]
    }
   ],
   "source": [
    "countries_table_path = f'{output_path}countries_table_{get_time()}'\n",
    "print(f'Writing airports_table to {countries_table_path}..')\n",
    "countries_table.write.parquet(countries_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `cities` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def make_iso_region(state):\n",
    "    return f'US-{state}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "city_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_data.createOrReplaceTempView('city_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_table = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        DISTINCT City AS city,\n",
    "        `State Code` AS state,\n",
    "        `Median Age` AS median_age,\n",
    "        `Male Population` AS population_m,\n",
    "        `Female Population` AS population_f,\n",
    "        `Total Population` AS population_total\n",
    "    FROM city_data\n",
    "    ORDER BY city, state\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_table = cities_table.withColumn('iso_region', make_iso_region('state'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------+------------+------------+----------------+----------+\n",
      "|   city|state|median_age|population_m|population_f|population_total|iso_region|\n",
      "+-------+-----+----------+------------+------------+----------------+----------+\n",
      "|Abilene|   TX|      31.3|       65212|       60664|          125876|     US-TX|\n",
      "|  Akron|   OH|      38.1|       96886|      100667|          197553|     US-OH|\n",
      "+-------+-----+----------+------------+------------+----------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cities_table.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing airports_table to s3a://udacity-data/capstone/output/cities_table_2020-6-21_3-48-3-125501..\n"
     ]
    }
   ],
   "source": [
    "cities_table_path = f'{output_path}cities_table_{get_time()}'\n",
    "print(f'Writing airports_table to {cities_table_path}..')\n",
    "# cities_table.write.parquet(cities_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `airports` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_data.createOrReplaceTempView('airport_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_table = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        a.name AS airport_name,\n",
    "        c.iso_country_id AS iso_country_id,\n",
    "        a.local_code AS local_code,\n",
    "        a.Iata_code AS iata_code,\n",
    "        a.coordinates AS coordinates,\n",
    "        a.type AS type\n",
    "    FROM airport_data AS a\n",
    "    JOIN countries_table AS c\n",
    "    ON (c.iso_country_alpha = a.iso_country)\n",
    "    WHERE iata_code IS NOT NULL AND iata_code != '-'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_table = airports_table.withColumn('airport_id', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+----------+---------+--------------------+-------------+----------+\n",
      "|        airport_name|iso_country_id|local_code|iata_code|         coordinates|         type|airport_id|\n",
      "+--------------------+--------------+----------+---------+--------------------+-------------+----------+\n",
      "|      Zaranj Airport|           004|      null|      ZAJ|61.865833, 30.972222|small_airport|         0|\n",
      "|Camp Bastion Airport|           004|      null|      OAZ|64.2246017456, 31...|small_airport|         1|\n",
      "+--------------------+--------------+----------+---------+--------------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airports_table.limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_table.createOrReplaceTempView('airports_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing airports_table to s3a://udacity-data/capstone/output/airports_table_2020-6-21_3-48-8-643791..\n"
     ]
    }
   ],
   "source": [
    "airports_table_path = f'{output_path}airports_table_{get_time()}'\n",
    "print(f'Writing airports_table to {airports_table_path}..')\n",
    "airport_table.write.parquet(airport_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build `immigration` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_data.createOrReplaceTempView('immigration_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_table = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        i.cicid AS immigration_id,\n",
    "        i.i94yr AS entry_year,\n",
    "        i.i94cit AS country_id,\n",
    "        a.airport_id AS airport_id,\n",
    "        i.arrdate AS arrival_date,\n",
    "        i.depdate AS departure_date,\n",
    "        i.biryear AS birth_year,\n",
    "        i.gender AS gender,\n",
    "        i.visatype AS visatype,\n",
    "        i.airline AS airline,\n",
    "        i.admnum AS admission_number\n",
    "    FROM immigration_data AS i\n",
    "    LEFT JOIN airports_table AS a ON i.i94port = a.iata_code\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_table = immigration_table.withColumn(\n",
    "    'immigration_id', immigration_table['immigration_id'].cast('int')).withColumn(\n",
    "    'entry_year', immigration_table['entry_year'].cast('int')).withColumn(\n",
    "    'country_id', immigration_table['country_id'].cast('int')).withColumn(\n",
    "    'arrival_date', immigration_table['arrival_date'].cast('int')).withColumn(\n",
    "    'departure_date', immigration_table['departure_date'].cast('int')).withColumn(\n",
    "    'birth_year', immigration_table['birth_year'].cast('int')).withColumn(\n",
    "    'admission_number', immigration_table['admission_number'].cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------\n",
      " immigration_id   | 5761355    \n",
      " entry_year       | 2016       \n",
      " country_id       | 297        \n",
      " airport_id       | 8629       \n",
      " arrival_date     | 20574      \n",
      " departure_date   | null       \n",
      " birth_year       | 1953       \n",
      " gender           | F          \n",
      " visatype         | B1         \n",
      " airline          | 348        \n",
      " admission_number | 2147483647 \n",
      "-RECORD 1----------------------\n",
      " immigration_id   | 5761356    \n",
      " entry_year       | 2016       \n",
      " country_id       | 297        \n",
      " airport_id       | 8629       \n",
      " arrival_date     | 20574      \n",
      " departure_date   | 20674      \n",
      " birth_year       | 1973       \n",
      " gender           | M          \n",
      " visatype         | B1         \n",
      " airline          | 348        \n",
      " admission_number | 2147483647 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_table.limit(2).show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing immigration_table to s3a://udacity-data/capstone/output/immigration_table_2020-6-21_3-48-47-97772..\n"
     ]
    }
   ],
   "source": [
    "immigration_table_path = f'{output_path}immigration_table_{get_time()}'\n",
    "print(f'Writing immigration_table to {immigration_table_path}..')\n",
    "# airport_table.write.parquet(airport_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data quality checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " \n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    "\n",
    "For the data quality checks we will be doing on the data, we will be doing unique checks for each check but as a common check will check first if they all have rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if all tables have data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries_count = countries_table.count()\n",
    "countries_count > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities_count = cities_table.count()\n",
    "cities_count > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports_count = airports_table.count()\n",
    "airports_count > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_count = immigration_table.count()\n",
    "immigration_count > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrity checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  `countries`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked 249 rows against processed data for countries table and found True\n"
     ]
    }
   ],
   "source": [
    "source_rows = country_data.count()\n",
    "processed_rows = countries_table.count()\n",
    "print(f'Checked {source_rows} rows against processed data for countries table and found {source_rows == processed_rows}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a side effect of our `JOIN` on this data? We have also stored data for countries that are not part of the I94 record for 2016 **OR** there is a difference in representation between stateless/invalid country codes. Reasons for why this was done that way are discussed in the last section.\n",
    "\n",
    "\n",
    "We will check next if every country in the table has an ISO country code - we check for this to ensure no country is unidentified by a unique ID. Why ISO ID and not I94 ID? This way we also check for countries that are not in the USA I94 Country Code data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 null rows in countries table\n"
     ]
    }
   ],
   "source": [
    "target_rows = countries_table.filter(countries_table.iso_country_id == 'null').count()\n",
    "print(f'Found {target_rows} null rows in countries table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  `cities`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked 2891 rows against processed data for cities table and found False\n"
     ]
    }
   ],
   "source": [
    "source_rows = city_data.count()\n",
    "processed_rows = cities_table.count()\n",
    "print(f'Checked {source_rows} rows against processed data for cities table and found {source_rows == processed_rows}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `cities`, our unique identifier is a combination of the city and state. We should also ensure we do not have any cities with no population or false total population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 null rows in cities table\n"
     ]
    }
   ],
   "source": [
    "target_rows = cities_table.filter('city IS null').count()\n",
    "target_rows += cities_table.filter('state IS null').count()\n",
    "\n",
    "print(f'Found {target_rows} null rows in cities table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_rows = cities_table.filter(cities_table.population_total == 0).count()\n",
    "target_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_check = cities_table.withColumn('total_check', cities_table.population_m + cities_table.population_f)\n",
    "target_rows = pop_check.filter(pop_check.population_total != pop_check.total_check).count()\n",
    "target_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. `airports`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked 55075 rows against processed data for airports table and found False\n",
      "45888 rows missing\n"
     ]
    }
   ],
   "source": [
    "source_rows = airport_data.count()\n",
    "processed_rows = airports_table.count()\n",
    "print(f'Checked {source_rows} rows against processed data for airports table and found {source_rows == processed_rows}')\n",
    "print(f'{source_rows - processed_rows} rows missing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we have way fewer rows than the source data? This is because we exclude all airports for which we do not have a valid `iata_code`. We use this code to match records in our immigration data. Without a valid ID to match, it does not make sense to store these airports, especially as some of them are extremely small or private airports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if any airports don't have a country assigned, in which case we can't join `countries` effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 null country rows in airports table\n"
     ]
    }
   ],
   "source": [
    "target_rows = airports_table.filter('iso_country_id IS null').count()\n",
    "print(f'Found {target_rows} null country rows in airports table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 unnamed airports\n"
     ]
    }
   ],
   "source": [
    "target_rows = airports_table.filter('name IS null').count()\n",
    "print(f'Found {target_rows} unnamed airports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. `immigration`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_rows = immigration_table.filter('immigration_id IS null').count()\n",
    "target_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- immigration_id: integer (nullable = true)\n",
      " |-- entry_year: integer (nullable = true)\n",
      " |-- country_id: integer (nullable = true)\n",
      " |-- airport_id: long (nullable = true)\n",
      " |-- arrival_date: integer (nullable = true)\n",
      " |-- departure_date: integer (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admission_number: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_table.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data dictionary \n",
    "\n",
    "`countries`\n",
    "\n",
    "* `i94_country_id`: ID for a country as stored in USA I94 records. Foreign key to `immigration`\n",
    "* `iso_country_id`: ID for a country as stored in ISO\n",
    "* `country_name`: Common name of the country\n",
    "* `region`: Region of the country i.e continent\n",
    "* `sub_region`: Subregion in the continent the country belongs to. eg: Southeast Asia\n",
    "* `iso_country_alpha`: Two character alphanumeric country representation. Foreign key to `airports`\n",
    "\n",
    "---\n",
    "`cities`\n",
    "\n",
    "* `city`: Common name of the city. Part of the candidate key to `airports`\n",
    "* `state`: Common name of the state the city is in. Part of the candidate key to `airports`\n",
    "* `median_age`: Median age of population as a floating point\n",
    "* `population_m`: Count of male population\n",
    "* `population_m`: Count of female population\n",
    "* `population_total`: Count of total population\n",
    "\n",
    "\n",
    "---\n",
    "`airports`\n",
    "\n",
    "* `airport_name`: Common name of the airport. Cannot be empty\n",
    "* `iso_country_id`: ISO country ID the airport is in\n",
    "* `local_code`: Local code reference for airport\n",
    "* `iata_code`: IATA code reference for the airport. Foreign key to `immigration`\n",
    "* `coordinates`: Location of airport as a latitude, longitude string\n",
    "* `type`: Class of airport\n",
    "* `airport_id`: Unique ID for airport\n",
    "\n",
    "---\n",
    "`immigration`\n",
    "\n",
    "* `immigration_id`: ID of entry on I94 records\n",
    "* `entry_year`: Year the entry was recorded\n",
    "* `country_id`: I94 country code for the country immigrant is from. Refers to `countries`\n",
    "* `airport_id`: Airport ID for airport the entry was recorded. Refers to `airports`\n",
    "* `arrival_date`: Date the arrival was recorded\n",
    "* `departure_date`: If departure, date it was recorded, else empty\n",
    "* `birth_year`: Birth year of immigrant\n",
    "* `gender`: M if male, F if female otherwise empty\n",
    "* `visatype`: Class of visa immigrant arrived on\n",
    "* `airline`: Airline the immigrant flew on for that particular entry\n",
    "* `admission_number`: Recorded if part of a group of arrivals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project write-up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "The motivation behind this was mainly to store immigration data in a data lake while also adding additional supporting data to it, namely airports, countries and cities data. One of the bigger thoughts behind the design was to try to not process this data in a way that it would be confined to USA immigration data. As such, the airport and countries list contains data outside of the USA. As much of the design was done to ensure that we can theoretically add immigration records from another country **OR** use a different immigration table while using the same dimension tables for it.\n",
    "\n",
    "\n",
    "Since the choice was to build a data lake, Spark seemed like a good option. In this case, immigration data for one year was used but we can easily scale up Spark to deal with data from multiple years. S3 was used to both store initial data and as an endpoint namely because this is assumed to be data that will persist for a long time.\n",
    "\n",
    "---\n",
    "\n",
    "* Propose how often the data should be updated and why.\n",
    "\n",
    "For the dimensions, it does not make sense to update it very often since the list of airports or countries will not change. However, our list is **not** complete and we do have a few unmatched immigration records in our data. As such, we can update our dimensions with more airports and countries as we find sources.\n",
    "\n",
    "For the immigration data, updating and staging it every month would be a good option. The USA gets a lot of immigrants, especially with the various visas so it's a good use of this pipeline to update regularly.\n",
    "\n",
    "---\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " \n",
    " A more powerful compute EMR cluster would be used for running Spark. From a storage perspective, we could also try to use a columnar store and partition our immigration records by country, by airport or by visa type.\n",
    " \n",
    " ---\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " \n",
    " Including steps in the pipeline to read data from the final S3 locations and run certain analytical queries would be a great thing to do for such a usecase. Additionally, an innovative approach might be to populate only the immigration records and update the dimensions with their sources once a month.\n",
    " \n",
    " ---\n",
    " * The database needed to be accessed by 100+ people.\n",
    "\n",
    "Our design choice to use S3 would still work in that case but controlling access would be problematic. Additionally, if 100+ people with different requirements need the data to be retreived fast, using a more traditional database or a columnar store would fit our need much better\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credit to the following authors for additional datasets on countries and country codes:**\n",
    "\n",
    "* https://github.com/lukes/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Credit to the following authors for inspiration for the project and additional datasets:**\n",
    "\n",
    "* https://github.com/jukkakansanaho"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
